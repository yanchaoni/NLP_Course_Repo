{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import spacy\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import en_core_web_sm\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 500\n",
    "max_vocab_size = 10000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    data = []\n",
    "    rate = []\n",
    "    for x in os.listdir(path):\n",
    "        rate.append(x.split(\".\")[0][-1])\n",
    "        with open(path+x, \"r\") as f:\n",
    "            data.extend(f.readlines())\n",
    "    return data, len(data), rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"aclImdb/\"\n",
    "train_data, n_pos, _ = get_data(path+\"train/pos/\")\n",
    "train_data.extend(get_data(path+\"train/neg/\")[0])\n",
    "n_neg = len(train_data) - n_pos\n",
    "train_targets = [1] * n_pos + [0] * n_neg\n",
    "test_data, n_pos, _ = get_data(path+\"test/pos/\")\n",
    "test_data.extend(get_data(path+\"test/neg/\")[0])\n",
    "n_neg = len(test_data) - n_pos\n",
    "test_targets = [1] * n_pos + [0] * n_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, train_targets, val_targets = train_test_split(train_data, train_targets, test_size=5000, random_state=42)\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump([train_data, val_data, train_targets, val_targets, test_data, test_targets], open(\"data/raw_data.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = en_core_web_sm.load()\n",
    "punctuations = string.punctuation\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to Ilya Kulikov\n",
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "def tokenize_dataset(dataset, gram=1):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    tokenizer = en_core_web_sm.load()     \n",
    "    for sample in tqdm_notebook(tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=2)):\n",
    "        tokens = lower_case_remove_punc(sample)\n",
    "        if gram > 1:\n",
    "            tokens += [\"\".join(tokens[i:i+gram]) for i in range(len(tokens)-gram)]\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "def tokenize_dataset_seq(dataset, gram):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in tqdm_notebook(dataset):\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        if gram > 1:\n",
    "            tokens += [\"\".join(tokens[i:i+gram]) for i in range(len(tokens)-gram)]\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "def nltk_tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    tokenizer = word_tokenize\n",
    "    for sample in tqdm_notebook(dataset):\n",
    "        parsed = tokenizer(sample)\n",
    "        tokens = [token.lower() for token in parsed if (token not in punctuations)]\n",
    "#         tokens = [token for token in sample]\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def _collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"  \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data = data.cuda()\n",
    "        lengths = lengths.cuda()\n",
    "        labels = labels.cuda()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learning_rate, num_epochs, optimizer, save=False, label=\"\", schedule=False, tolerance=6):\n",
    "    criterion = torch.nn.CrossEntropyLoss()  \n",
    "    hist = []\n",
    "    best_val = 0\n",
    "    fail_cnt = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data.cuda(), lengths.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            hist.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                if schedule:\n",
    "                    scheduler.step(val_acc)\n",
    "                if val_acc > best_val:\n",
    "                    fail_cnt = 0\n",
    "                    if save:\n",
    "                        torch.save(model.state_dict(), 'model' + label + '.ckpt')\n",
    "                    best_val = val_acc\n",
    "                    print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "                else:\n",
    "                    fail_cnt += 1\n",
    "                    print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, failed {} times'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc, fail_cnt))\n",
    "            if fail_cnt == tolerance:\n",
    "                break\n",
    "            \n",
    "    return hist, best_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[train_data, val_data, train_targets, val_targets, test_data, test_targets] = pkl.load(open(\"data/raw_data.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram = 2\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_data, gram=gram)\n",
    "# val_data_tokens, _ = nltk_tokenize_dataset(val_data)\n",
    "pkl.dump(val_data_tokens, open(\"val_data_\"+str(gram)+\"tokens.p\", \"wb\"))\n",
    "\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(train_data, gram=gram)\n",
    "# train_data_tokens, all_train_tokens = nltk_tokenize_dataset(train_data)\n",
    "pkl.dump(train_data_tokens, open(\"train_data_\"+str(gram)+\"_tokens.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_\"+str(gram)+\"_tokens.p\", \"wb\"))\n",
    "\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset_seq(test_data, gram=gram)\n",
    "pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "# test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "# print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = ImdbDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = ImdbDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 50\n",
    "learning_rate = 0.005\n",
    "num_epochs = 100\n",
    "tolerance = 15\n",
    "model = BagOfWords(len(id2token), emb_dim).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=6, factor=0.1)\n",
    "hist, best_val = train(learning_rate, num_epochs, optimizer, schedule=True, tolerance=tolerance, \n",
    "                       save=True, label=\"LoPun-unigram-Adam-0.005-Ann-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"nltkLoPun-unigram-Adam-0.005-Ann-100\"] = {\"hist\": hist, \"best_val\": best_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sorted_results = sorted(list(zip(*(results.keys(), results.values()))), key=lambda x: x[1]['best_val'])\n",
    "for config, res in sorted_results:\n",
    "    info = config.split(\"-\")\n",
    "    if (info[0] == \"LoPun\") and (info[1] == \"unigram\"):\n",
    "        print(\"{} & {} & {} & {} \\\\\\\\\".format(info[2], info[3], info[4], res[\"best_val\"]))\n",
    "        plt.plot(res[\"hist\"][::50], label=\"{}, lr={}, {}\".format(info[2], info[3], info[4]))\n",
    "        plt.legend()\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"steps/50\")\n",
    "        plt.title(\"Learning Curve of different optimizer configurations\")\n",
    "        plt.savefig(\"training_curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(results, open(\"results.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "test_dataset = ImdbDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=_collate_func,\n",
    "                                           shuffle=False)\n",
    "correct_cnt, wrong_cnt = 0, 0\n",
    "correct_list = []\n",
    "wrong_list = []\n",
    "prediction = []\n",
    "label = []\n",
    "model.eval()\n",
    "for data, lengths, labels in test_loader:\n",
    "    data = data.cuda()\n",
    "    lengths = lengths.cuda()\n",
    "    labels = labels.cuda()\n",
    "    data_batch, length_batch, label_batch = data, lengths, labels\n",
    "    outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "    predicted = outputs.max(1, keepdim=True)[1]\n",
    "    prediction.append(predicted.data[0])\n",
    "    label.append(labels.data[0])\n",
    "    \n",
    "    correct = predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    if (correct == 1) and (correct_cnt < 3):\n",
    "        if correct_cnt == 2:\n",
    "            if labels.data[0].item() == 1:\n",
    "                continue\n",
    "        correct_cnt  += 1\n",
    "        correct_list.append((data.data[0], labels.data[0]))\n",
    "    if (correct == 0) and (wrong_cnt < 3):\n",
    "        if wrong_cnt == 2:\n",
    "            if labels.data[0].item() == 1:\n",
    "                continue\n",
    "        wrong_cnt += 1\n",
    "        wrong_list.append((data.data[0], labels.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_table = {\"00\":0, \"01\":0, \"10\":0, \"11\":0}\n",
    "for i, j in zip(prediction, label):\n",
    "    truth_table[str(i.item())+str(j.item())] += 1\n",
    "truth_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in correct_list:\n",
    "    print(\"\\item label:{}\".format(ex[1]), \" \".join([id2token[i] for i in ex[0] if i > 0]).replace(\"<\", \" $<$ \").replace(\">\", \" $>$ \"))\n",
    "print(\"-\" * 20)\n",
    "for ex in wrong_list:\n",
    "    print(\"\\item label:{}\".format(ex[1]), \" \".join([id2token[i] for i in ex[0] if i > 0]).replace(\"<\", \" $<$ \").replace(\">\", \" $>$ \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
